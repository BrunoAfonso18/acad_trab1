{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0167c09",
   "metadata": {
    "papermill": {
     "duration": 0.054838,
     "end_time": "2023-09-13T18:55:29.487191",
     "exception": false,
     "start_time": "2023-09-13T18:55:29.432353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"contents_tabel\"></a>    \n",
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:130%; text-align:left\">\n",
    "\n",
    "<h2 align=\"left\"><font color=#ff6200>Table of Contents:</font></h2>\n",
    "    \n",
    "* [Step 1 | Setup and Initialization](#setup)\n",
    "    - [Step 1.1 | Importing Necessary Libraries](#libraries) \n",
    "    - [Step 1.2 | Loading the Dataset](#load_dataset)\n",
    "* [Step 2 | Initial Data Analysis](#initial_analysis) \n",
    "    - [Step 2.1 | Dataset Overview](#overview) \n",
    "    - [Step 2.2 | Summary Statistics](#statistics) \n",
    "* [Step 3 | Data Cleaning & Transformation](#data_cleaning)\n",
    "    - [Step 3.1 | Handling Missing Values](#missing_values)\n",
    "    - [Step 3.2 | Handling Duplicates](#duplicates)\n",
    "    - [Step 3.3 | Treating Cancelled Transactions](#InvoiceNo_cleaning)\n",
    "    - [Step 3.4 | Correcting StockCode Anomalies](#StockCode_cleaning)\n",
    "    - [Step 3.5 | Cleaning Description Column](#Description_cleaning)\n",
    "    - [Step 3.6 | Treating Zero Unit Prices](#UnitPrice_cleaning)\n",
    "    - [Step 3.7 | Outlier Treatment](#outlier_cleaning)\n",
    "* [Step 4 | Feature Engineering](#feature_engineering)\n",
    "    - [Step 4.1 | RFM Features](#rfm_features)\n",
    "        - [Step 4.1.1 | Recency (R)](#recency) \n",
    "        - [Step 4.1.2 | Frequency (F)](#frequency)\n",
    "        - [Step 4.1.3 | Monetary (M)](#monetary)\n",
    "    - [Step 4.2 | Product Diversity](#product_diversity)\n",
    "    - [Step 4.3 | Behavioral Features](#behaviroal_features)\n",
    "    - [Step 4.4 | Geographic Features](#geographical_features)\n",
    "    - [Step 4.5 | Cancellation Insights](#cancellation_insights) \n",
    "    - [Step 4.6 | Seasonality & Trends](#seasonality_trends) \n",
    "* [Step 5 | Outlier Detection and Treatment](#outlier_detection)\n",
    "* [Step 6 | Correlation Analysis](#correlation)\n",
    "* [Step 7 | Feature Scaling](#scaling)\n",
    "* [Step 8 | Dimensionality Reduction](#pca)\n",
    "* [Step 9 | K-Means Clustering](#kmeans) \n",
    "    - [Step 9.1 | Determining the Optimal Number of Clusters](#optimal_k) \n",
    "        - [Step 9.1.1 | Elbow Method](#elbow)\n",
    "        - [Step 9.1.2 | Silhouette Method](#silhouette)\n",
    "    - [Step 9.2 | Clustering Model - K-means](#kmeans_model)\n",
    "* [Step 10 | Clustering Evaluation](#evaluation)  \n",
    "    - [Step 10.1 | 3D Visualization of Top Principal Components](#3d_visualization)\n",
    "    - [Step 10.2 | Cluster Distribution Visualization](#cluster_distributuion) \n",
    "    - [Step 10.3 | Evaluation Metrics](#evaluations_metrics)\n",
    "* [Step 11 | Cluster Analysis and Profiling](#profiling)\n",
    "    - [Step 11.1 | Radar Chart Approach](#radar_chart)\n",
    "    - [Step 11.2 | Histogram Chart Approach](#histogram)\n",
    "* [Step 12 | Recommendation System](#recommendation_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bff998",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.058194,
     "end_time": "2023-09-13T18:55:29.602828",
     "exception": false,
     "start_time": "2023-09-13T18:55:29.544634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 align=\"left\"><font color=#ff6200>Let's get started:</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e6dcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.055048,
     "end_time": "2023-09-13T18:55:29.713486",
     "exception": false,
     "start_time": "2023-09-13T18:55:29.658438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 1 | Setup and Initialization</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e393a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.054748,
     "end_time": "2023-09-13T18:55:29.823414",
     "exception": false,
     "start_time": "2023-09-13T18:55:29.768666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"libraries\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 1.1 |</span><span style='color:#ff6200'> Importing Necessary Libraries</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0d4f2",
   "metadata": {
    "papermill": {
     "duration": 3.71761,
     "end_time": "2023-09-13T18:55:33.705058",
     "exception": false,
     "start_time": "2023-09-13T18:55:29.987448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pip install yellowbrick\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import colors as mcolors\n",
    "from scipy.stats import linregress\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a57e4",
   "metadata": {
    "papermill": {
     "duration": 0.326571,
     "end_time": "2023-09-13T18:55:34.104602",
     "exception": false,
     "start_time": "2023-09-13T18:55:33.778031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Plotly for use in the notebook\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f647ef",
   "metadata": {
    "papermill": {
     "duration": 0.065754,
     "end_time": "2023-09-13T18:55:34.225966",
     "exception": false,
     "start_time": "2023-09-13T18:55:34.160212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure Seaborn plot styles: Set background color and use dark grid\n",
    "sns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859bfb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.05608,
     "end_time": "2023-09-13T18:55:34.336948",
     "exception": false,
     "start_time": "2023-09-13T18:55:34.280868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"load_dataset\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 1.2 |</span><span style='color:#ff6200'> Loading the Dataset</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa0be4",
   "metadata": {
    "papermill": {
     "duration": 1.815595,
     "end_time": "2023-09-13T18:55:36.317782",
     "exception": false,
     "start_time": "2023-09-13T18:55:34.502187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./retail/retail.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e629c03-8eb6-4e08-94a5-9d5a67faa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275d113",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.05478,
     "end_time": "2023-09-13T18:55:36.539387",
     "exception": false,
     "start_time": "2023-09-13T18:55:36.484607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"initial_analysis\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 2 | Initial Data Analysis</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc056c8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.054688,
     "end_time": "2023-09-13T18:55:36.764128",
     "exception": false,
     "start_time": "2023-09-13T18:55:36.709440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"overview\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 2.1 |</span><span style='color:#ff6200'> Dataset Overview</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec4265",
   "metadata": {
    "papermill": {
     "duration": 0.098838,
     "end_time": "2023-09-13T18:55:37.031877",
     "exception": false,
     "start_time": "2023-09-13T18:55:36.933039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d408d719",
   "metadata": {
    "papermill": {
     "duration": 0.367369,
     "end_time": "2023-09-13T18:55:37.454997",
     "exception": false,
     "start_time": "2023-09-13T18:55:37.087628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606eacb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.057565,
     "end_time": "2023-09-13T18:55:37.681334",
     "exception": false,
     "start_time": "2023-09-13T18:55:37.623769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"statistics\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 2.2 |</span><span style='color:#ff6200'> Summary Statistics</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa41f6",
   "metadata": {
    "papermill": {
     "duration": 0.177485,
     "end_time": "2023-09-13T18:55:38.029051",
     "exception": false,
     "start_time": "2023-09-13T18:55:37.851566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summary statistics for numerical variables\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8e05d",
   "metadata": {
    "papermill": {
     "duration": 0.843701,
     "end_time": "2023-09-13T18:55:38.930130",
     "exception": false,
     "start_time": "2023-09-13T18:55:38.086429",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summary statistics for categorical variables\n",
    "df.describe(include='object').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987c3f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.058377,
     "end_time": "2023-09-13T18:55:39.164241",
     "exception": false,
     "start_time": "2023-09-13T18:55:39.105864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"data_cleaning\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 3 |  Data Cleaning & Transformation</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352c8a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.05663,
     "end_time": "2023-09-13T18:55:39.395575",
     "exception": false,
     "start_time": "2023-09-13T18:55:39.338945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"missing_values\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 3.1 |</span><span style='color:#ff6200'> Handling Missing Values</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6abe6",
   "metadata": {
    "papermill": {
     "duration": 0.983847,
     "end_time": "2023-09-13T18:55:40.553710",
     "exception": false,
     "start_time": "2023-09-13T18:55:39.569863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating the percentage of missing values for each column\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data[missing_data > 0] / df.shape[0]) * 100\n",
    "\n",
    "# Prepare values\n",
    "missing_percentage.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# Plot the barh chart\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "ax.barh(missing_percentage.index, missing_percentage, color='#ff6200')\n",
    "\n",
    "# Annotate the values and indexes\n",
    "for i, (value, name) in enumerate(zip(missing_percentage, missing_percentage.index)):\n",
    "    ax.text(value+0.5, i, f\"{value:.2f}%\", ha='left', va='center', fontweight='bold', fontsize=18, color='black')\n",
    "\n",
    "# Set x-axis limit\n",
    "ax.set_xlim([0, 40])\n",
    "\n",
    "# Add title and xlabel\n",
    "plt.title(\"Percentage of Missing Values\", fontweight='bold', fontsize=22)\n",
    "plt.xlabel('Percentages (%)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec499ed",
   "metadata": {
    "papermill": {
     "duration": 0.161893,
     "end_time": "2023-09-13T18:55:40.910774",
     "exception": false,
     "start_time": "2023-09-13T18:55:40.748881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting rows with missing values in 'CustomerID' or 'Description' columns\n",
    "df[df['CustomerID'].isnull() | df['Description'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2cf0c",
   "metadata": {
    "papermill": {
     "duration": 0.200287,
     "end_time": "2023-09-13T18:55:41.174169",
     "exception": false,
     "start_time": "2023-09-13T18:55:40.973882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing rows with missing values in 'CustomerID' and 'Description' columns\n",
    "df = df.dropna(subset=['CustomerID', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a2e89",
   "metadata": {
    "papermill": {
     "duration": 0.283735,
     "end_time": "2023-09-13T18:55:41.515200",
     "exception": false,
     "start_time": "2023-09-13T18:55:41.231465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verifying the removal of missing values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01227519-ed8a-48ca-a049-2117c461cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6625a-a15d-483f-b3e3-6e967352bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object').T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29966208",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.056893,
     "end_time": "2023-09-13T18:55:41.630113",
     "exception": false,
     "start_time": "2023-09-13T18:55:41.573220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"duplicates\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 3.2 |</span><span style='color:#ff6200'> Handling Duplicates</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d579fb",
   "metadata": {
    "papermill": {
     "duration": 0.505381,
     "end_time": "2023-09-13T18:55:42.311188",
     "exception": false,
     "start_time": "2023-09-13T18:55:41.805807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding duplicate rows (keeping all instances)\n",
    "duplicate_rows = df[df.duplicated(keep=False)]\n",
    "\n",
    "# Sorting the data by certain columns to see the duplicate rows next to each other\n",
    "duplicate_rows_sorted = duplicate_rows.sort_values(by=['InvoiceNo', 'StockCode', 'Description', 'CustomerID', 'Quantity'])\n",
    "\n",
    "# Displaying the first 10 records\n",
    "duplicate_rows_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2b098",
   "metadata": {
    "papermill": {
     "duration": 0.854262,
     "end_time": "2023-09-13T18:55:43.343966",
     "exception": false,
     "start_time": "2023-09-13T18:55:42.489704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying the number of duplicate rows\n",
    "print(f\"The dataset contains {df.duplicated().sum()} duplicate rows that need to be removed.\")\n",
    "\n",
    "# Removing duplicate rows\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96102471",
   "metadata": {
    "papermill": {
     "duration": 0.073206,
     "end_time": "2023-09-13T18:55:43.478411",
     "exception": false,
     "start_time": "2023-09-13T18:55:43.405205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the number of rows in the dataframe\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff4f89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.060407,
     "end_time": "2023-09-13T18:55:43.599222",
     "exception": false,
     "start_time": "2023-09-13T18:55:43.538815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"InvoiceNo_cleaning\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 3.3 |</span><span style='color:#ff6200'> Treating Cancelled Transactions</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e11e75",
   "metadata": {
    "papermill": {
     "duration": 0.486159,
     "end_time": "2023-09-13T18:55:44.294362",
     "exception": false,
     "start_time": "2023-09-13T18:55:43.808203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter out the rows with InvoiceNo starting with \"C\" and create a new column indicating the transaction status\n",
    "df['Transaction_Status'] = np.where(df['InvoiceNo'].astype(str).str.startswith('C'), 'Cancelled', 'Completed')\n",
    "\n",
    "# Analyze the characteristics of these rows (considering the new column)\n",
    "cancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']\n",
    "cancelled_transactions.describe().drop('CustomerID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1427b",
   "metadata": {
    "papermill": {
     "duration": 0.073523,
     "end_time": "2023-09-13T18:55:44.671722",
     "exception": false,
     "start_time": "2023-09-13T18:55:44.598199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding the percentage of cancelled transactions\n",
    "print(cancelled_transactions.shape[0])\n",
    "print(df.shape[0])\n",
    "cancelled_percentage = (cancelled_transactions.shape[0] / df.shape[0]) * 100\n",
    "\n",
    "# Printing the percentage of cancelled transactions\n",
    "print(f\"The percentage of cancelled transactions in the dataset is: {cancelled_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00cc10e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.060775,
     "end_time": "2023-09-13T18:55:44.793574",
     "exception": false,
     "start_time": "2023-09-13T18:55:44.732799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"StockCode_cleaning\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 3.4 |</span><span style='color:#ff6200'> Correcting StockCode Anomalies</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8eb6d4",
   "metadata": {
    "papermill": {
     "duration": 0.116377,
     "end_time": "2023-09-13T18:55:45.095198",
     "exception": false,
     "start_time": "2023-09-13T18:55:44.978821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding the number of unique stock codes\n",
    "unique_stock_codes = df['StockCode'].nunique()\n",
    "\n",
    "# Printing the number of unique stock codes\n",
    "print(f\"The number of unique stock codes in the dataset is: {unique_stock_codes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68693a",
   "metadata": {
    "papermill": {
     "duration": 0.623504,
     "end_time": "2023-09-13T18:55:45.780969",
     "exception": false,
     "start_time": "2023-09-13T18:55:45.157465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding the top 10 most frequent stock codes\n",
    "top_10_stock_codes = df['StockCode'].value_counts(normalize=True).head(10) * 100\n",
    "\n",
    "# Plotting the top 10 most frequent stock codes\n",
    "plt.figure(figsize=(12, 5))\n",
    "top_10_stock_codes.plot(kind='barh', color='#ff6200')\n",
    "\n",
    "# Adding the percentage frequency on the bars\n",
    "for index, value in enumerate(top_10_stock_codes):\n",
    "    plt.text(value, index+0.25, f'{value:.2f}%', fontsize=10)\n",
    "\n",
    "plt.title('Top 10 Most Frequent Stock Codes')\n",
    "plt.xlabel('Percentage Frequency (%)')\n",
    "plt.ylabel('Stock Codes')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39dde0",
   "metadata": {
    "papermill": {
     "duration": 0.124219,
     "end_time": "2023-09-13T18:55:46.210927",
     "exception": false,
     "start_time": "2023-09-13T18:55:46.086708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding the number of numeric characters in each unique stock code\n",
    "unique_stock_codes = df['StockCode'].unique()\n",
    "numeric_char_counts_in_unique_codes = pd.Series(unique_stock_codes).apply(lambda x: sum(c.isdigit() for c in str(x))).value_counts()\n",
    "\n",
    "# Printing the value counts for unique stock codes\n",
    "print(\"Value counts of numeric character frequencies in unique stock codes:\")\n",
    "print(\"-\"*70)\n",
    "print(numeric_char_counts_in_unique_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77541192",
   "metadata": {
    "papermill": {
     "duration": 0.078998,
     "end_time": "2023-09-13T18:55:46.475514",
     "exception": false,
     "start_time": "2023-09-13T18:55:46.396516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and printing the stock codes with 0 and 1 numeric characters\n",
    "anomalous_stock_codes = [code for code in unique_stock_codes if sum(c.isdigit() for c in str(code)) in (0, 1)]\n",
    "\n",
    "# Printing each stock code on a new line\n",
    "print(\"Anomalous stock codes:\")\n",
    "print(\"-\"*22)\n",
    "for code in anomalous_stock_codes:\n",
    "    print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9d357",
   "metadata": {
    "papermill": {
     "duration": 0.103691,
     "end_time": "2023-09-13T18:55:46.767256",
     "exception": false,
     "start_time": "2023-09-13T18:55:46.663565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating the percentage of records with these stock codes\n",
    "percentage_anomalous = (df['StockCode'].isin(anomalous_stock_codes).sum() / len(df)) * 100\n",
    "\n",
    "# Printing the percentage\n",
    "print(f\"The percentage of records with anomalous stock codes in the dataset is: {percentage_anomalous:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db59942",
   "metadata": {
    "papermill": {
     "duration": 0.158751,
     "end_time": "2023-09-13T18:55:47.364629",
     "exception": false,
     "start_time": "2023-09-13T18:55:47.205878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing rows with anomalous stock codes from the dataset\n",
    "df = df[~df['StockCode'].isin(anomalous_stock_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3dda95",
   "metadata": {
    "papermill": {
     "duration": 0.070041,
     "end_time": "2023-09-13T18:55:47.495154",
     "exception": false,
     "start_time": "2023-09-13T18:55:47.425113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the number of rows in the dataframe\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523c417",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.059584,
     "end_time": "2023-09-13T18:55:47.615935",
     "exception": false,
     "start_time": "2023-09-13T18:55:47.556351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Description_cleaning\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 3.5 |</span><span style='color:#ff6200'> Cleaning Description Column</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4eebb",
   "metadata": {
    "papermill": {
     "duration": 0.906535,
     "end_time": "2023-09-13T18:55:48.705153",
     "exception": false,
     "start_time": "2023-09-13T18:55:47.798618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the occurrence of each unique description and sort them\n",
    "description_counts = df['Description'].value_counts()\n",
    "\n",
    "# Get the top 30 descriptions\n",
    "top_30_descriptions = description_counts[:30]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(top_30_descriptions.index[::-1], top_30_descriptions.values[::-1], color='#ff6200')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Description')\n",
    "plt.title('Top 30 Most Frequent Descriptions')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa4a45",
   "metadata": {
    "papermill": {
     "duration": 0.14653,
     "end_time": "2023-09-13T18:55:49.044148",
     "exception": false,
     "start_time": "2023-09-13T18:55:48.897618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find unique descriptions containing lowercase characters\n",
    "lowercase_descriptions = df['Description'].unique()\n",
    "lowercase_descriptions = [desc for desc in lowercase_descriptions if any(char.islower() for char in desc)]\n",
    "\n",
    "# Print the unique descriptions containing lowercase characters\n",
    "print(\"The unique descriptions containing lowercase characters are:\")\n",
    "print(\"-\"*60)\n",
    "for desc in lowercase_descriptions:\n",
    "    print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ed68d",
   "metadata": {
    "papermill": {
     "duration": 0.436004,
     "end_time": "2023-09-13T18:55:49.899055",
     "exception": false,
     "start_time": "2023-09-13T18:55:49.463051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_related_descriptions = [\"Next Day Carriage\", \"High Resolution Image\"]\n",
    "\n",
    "# Calculate the percentage of records with service-related descriptions\n",
    "service_related_percentage = df[df['Description'].isin(service_related_descriptions)].shape[0] / df.shape[0] * 100\n",
    "\n",
    "# Print the percentage of records with service-related descriptions\n",
    "print(f\"The percentage of records with service-related descriptions in the dataset is: {service_related_percentage:.2f}%\")\n",
    "\n",
    "# Remove rows with service-related information in the description\n",
    "df = df[~df['Description'].isin(service_related_descriptions)]\n",
    "\n",
    "# Standardize the text to uppercase to maintain uniformity across the dataset\n",
    "df['Description'] = df['Description'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30511fa4",
   "metadata": {
    "papermill": {
     "duration": 0.074834,
     "end_time": "2023-09-13T18:55:50.038775",
     "exception": false,
     "start_time": "2023-09-13T18:55:49.963941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the number of rows in the dataframe\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464073aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.064272,
     "end_time": "2023-09-13T18:55:50.169305",
     "exception": false,
     "start_time": "2023-09-13T18:55:50.105033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"UnitPrice_cleaning\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 3.6 |</span><span style='color:#ff6200'> Treating Zero Unit Prices</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5299a",
   "metadata": {
    "papermill": {
     "duration": 0.097583,
     "end_time": "2023-09-13T18:55:50.456794",
     "exception": false,
     "start_time": "2023-09-13T18:55:50.359211",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['UnitPrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c5b8b",
   "metadata": {
    "papermill": {
     "duration": 0.09384,
     "end_time": "2023-09-13T18:55:50.748583",
     "exception": false,
     "start_time": "2023-09-13T18:55:50.654743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['UnitPrice']==0].describe()[['Quantity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd2d24",
   "metadata": {
    "papermill": {
     "duration": 0.125633,
     "end_time": "2023-09-13T18:55:51.201038",
     "exception": false,
     "start_time": "2023-09-13T18:55:51.075405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing records with a unit price of zero to avoid potential data entry errors\n",
    "df = df[df['UnitPrice'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd347600-c15c-42ac-ba3b-b0a956caa25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d3071",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.064292,
     "end_time": "2023-09-13T18:55:51.330774",
     "exception": false,
     "start_time": "2023-09-13T18:55:51.266482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"outlier_cleaning\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 3.7 |</span><span style='color:#ff6200'> Outlier Treatment</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2e314",
   "metadata": {
    "papermill": {
     "duration": 0.075851,
     "end_time": "2023-09-13T18:55:51.600414",
     "exception": false,
     "start_time": "2023-09-13T18:55:51.524563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resetting the index of the cleaned dataset\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a70e2",
   "metadata": {
    "papermill": {
     "duration": 0.077309,
     "end_time": "2023-09-13T18:55:51.743412",
     "exception": false,
     "start_time": "2023-09-13T18:55:51.666103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the number of rows in the dataframe\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0423204",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.065855,
     "end_time": "2023-09-13T18:55:51.875480",
     "exception": false,
     "start_time": "2023-09-13T18:55:51.809625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"feature_engineering\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 4 | Feature Engineering</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996035e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.068241,
     "end_time": "2023-09-13T18:55:52.142187",
     "exception": false,
     "start_time": "2023-09-13T18:55:52.073946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"rfm_features\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 4.1 |</span><span style='color:#ff6200'> RFM Features</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49247c5e",
   "metadata": {
    "papermill": {
     "duration": 0.065054,
     "end_time": "2023-09-13T18:55:52.406366",
     "exception": false,
     "start_time": "2023-09-13T18:55:52.341312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"recency\"></a>\n",
    "## <b><span style='color:#fcc36d'>Step 4.1.1 |</span><span style='color:#ff6200'> Recency (R)</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e426e9",
   "metadata": {
    "papermill": {
     "duration": 1.173973,
     "end_time": "2023-09-13T18:55:53.778716",
     "exception": false,
     "start_time": "2023-09-13T18:55:52.604743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert InvoiceDate to datetime type\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Convert InvoiceDate to datetime and extract only the date\n",
    "df['InvoiceDay'] = df['InvoiceDate'].dt.date\n",
    "\n",
    "# Find the most recent purchase date for each customer\n",
    "customer_data = df.groupby('CustomerID')['InvoiceDay'].max().reset_index()\n",
    "\n",
    "# Find the most recent date in the entire dataset\n",
    "most_recent_date = df['InvoiceDay'].max()\n",
    "\n",
    "# Convert InvoiceDay to datetime type before subtraction\n",
    "customer_data['InvoiceDay'] = pd.to_datetime(customer_data['InvoiceDay'])\n",
    "most_recent_date = pd.to_datetime(most_recent_date)\n",
    "\n",
    "# Calculate the number of days since the last purchase for each customer\n",
    "customer_data['Days_Since_Last_Purchase'] = (most_recent_date - customer_data['InvoiceDay']).dt.days\n",
    "\n",
    "# Remove the InvoiceDay column\n",
    "customer_data.drop(columns=['InvoiceDay'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ba7d2",
   "metadata": {
    "papermill": {
     "duration": 0.082016,
     "end_time": "2023-09-13T18:55:54.058539",
     "exception": false,
     "start_time": "2023-09-13T18:55:53.976523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5ec72",
   "metadata": {
    "papermill": {
     "duration": 0.063722,
     "end_time": "2023-09-13T18:55:54.318090",
     "exception": false,
     "start_time": "2023-09-13T18:55:54.254368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"frequency\"></a>\n",
    "## <b><span style='color:#fcc36d'>Step 4.1.2 |</span><span style='color:#ff6200'> Frequency (F)</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8b928",
   "metadata": {
    "papermill": {
     "duration": 0.187211,
     "end_time": "2023-09-13T18:55:54.703665",
     "exception": false,
     "start_time": "2023-09-13T18:55:54.516454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of transactions made by each customer\n",
    "total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
    "total_transactions.rename(columns={'InvoiceNo': 'Total_Transactions'}, inplace=True)\n",
    "\n",
    "# Calculate the total number of products purchased by each customer\n",
    "total_products_purchased = df.groupby('CustomerID')['Quantity'].sum().reset_index()\n",
    "total_products_purchased.rename(columns={'Quantity': 'Total_Products_Purchased'}, inplace=True)\n",
    "\n",
    "# Merge the new features into the customer_data dataframe\n",
    "customer_data = pd.merge(customer_data, total_transactions, on='CustomerID')\n",
    "customer_data = pd.merge(customer_data, total_products_purchased, on='CustomerID')\n",
    "\n",
    "# Display the first few rows of the customer_data dataframe\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3261c",
   "metadata": {
    "papermill": {
     "duration": 0.085369,
     "end_time": "2023-09-13T18:55:54.854478",
     "exception": false,
     "start_time": "2023-09-13T18:55:54.769109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"monetary\"></a>\n",
    "## <b><span style='color:#fcc36d'>Step 4.1.3 |</span><span style='color:#ff6200'> Monetary (M)</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131e718",
   "metadata": {
    "papermill": {
     "duration": 0.121753,
     "end_time": "2023-09-13T18:55:55.178675",
     "exception": false,
     "start_time": "2023-09-13T18:55:55.056922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the total spend by each customer\n",
    "df['Total_Spend'] = df['UnitPrice'] * df['Quantity']\n",
    "total_spend = df.groupby('CustomerID')['Total_Spend'].sum().reset_index()\n",
    "\n",
    "# Calculate the average transaction value for each customer\n",
    "average_transaction_value = total_spend.merge(total_transactions, on='CustomerID')\n",
    "average_transaction_value['Average_Transaction_Value'] = average_transaction_value['Total_Spend'] / average_transaction_value['Total_Transactions']\n",
    "\n",
    "# Merge the new features into the customer_data dataframe\n",
    "customer_data = pd.merge(customer_data, total_spend, on='CustomerID')\n",
    "customer_data = pd.merge(customer_data, average_transaction_value[['CustomerID', 'Average_Transaction_Value']], on='CustomerID')\n",
    "\n",
    "# Display the first few rows of the customer_data dataframe\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c28443",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.067521,
     "end_time": "2023-09-13T18:55:55.315371",
     "exception": false,
     "start_time": "2023-09-13T18:55:55.247850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"product_diversity\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 4.2 |</span><span style='color:#ff6200'> Product Diversity</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ab01f",
   "metadata": {
    "papermill": {
     "duration": 0.251802,
     "end_time": "2023-09-13T18:55:55.767945",
     "exception": false,
     "start_time": "2023-09-13T18:55:55.516143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the number of unique products purchased by each customer\n",
    "unique_products_purchased = df.groupby('CustomerID')['StockCode'].nunique().reset_index()\n",
    "unique_products_purchased.rename(columns={'StockCode': 'Unique_Products_Purchased'}, inplace=True)\n",
    "\n",
    "# Merge the new feature into the customer_data dataframe\n",
    "customer_data = pd.merge(customer_data, unique_products_purchased, on='CustomerID')\n",
    "\n",
    "# Display the first few rows of the customer_data dataframe\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5beaa9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.068876,
     "end_time": "2023-09-13T18:55:55.902838",
     "exception": false,
     "start_time": "2023-09-13T18:55:55.833962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"behaviroal_features\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 4.3 |</span><span style='color:#ff6200'> Behavioral Features</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25325c84",
   "metadata": {
    "papermill": {
     "duration": 7.324267,
     "end_time": "2023-09-13T18:56:03.428738",
     "exception": false,
     "start_time": "2023-09-13T18:55:56.104471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract day of week and hour from InvoiceDate\n",
    "df['Day_Of_Week'] = df['InvoiceDate'].dt.dayofweek\n",
    "df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "\n",
    "# Calculate the average number of days between consecutive purchases\n",
    "days_between_purchases = df.groupby('CustomerID')['InvoiceDay'].apply(lambda x: (x.diff().dropna()).apply(lambda y: y.days))\n",
    "average_days_between_purchases = days_between_purchases.groupby('CustomerID').mean().reset_index()\n",
    "average_days_between_purchases.rename(columns={'InvoiceDay': 'Average_Days_Between_Purchases'}, inplace=True)\n",
    "\n",
    "# Find the favorite shopping day of the week\n",
    "favorite_shopping_day = df.groupby(['CustomerID', 'Day_Of_Week']).size().reset_index(name='Count')\n",
    "favorite_shopping_day = favorite_shopping_day.loc[favorite_shopping_day.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Day_Of_Week']]\n",
    "\n",
    "# Find the favorite shopping hour of the day\n",
    "favorite_shopping_hour = df.groupby(['CustomerID', 'Hour']).size().reset_index(name='Count')\n",
    "favorite_shopping_hour = favorite_shopping_hour.loc[favorite_shopping_hour.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Hour']]\n",
    "\n",
    "# Merge the new features into the customer_data dataframe\n",
    "customer_data = pd.merge(customer_data, average_days_between_purchases, on='CustomerID')\n",
    "customer_data = pd.merge(customer_data, favorite_shopping_day, on='CustomerID')\n",
    "customer_data = pd.merge(customer_data, favorite_shopping_hour, on='CustomerID')\n",
    "\n",
    "# Display the first few rows of the customer_data dataframe\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dcba45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.068396,
     "end_time": "2023-09-13T18:56:03.569155",
     "exception": false,
     "start_time": "2023-09-13T18:56:03.500759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"geographical_features\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 4.4 |</span><span style='color:#ff6200'> Geographic Features</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c5883",
   "metadata": {
    "papermill": {
     "duration": 0.150849,
     "end_time": "2023-09-13T18:56:03.944819",
     "exception": false,
     "start_time": "2023-09-13T18:56:03.793970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Country'].value_counts(normalize=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d45c4",
   "metadata": {
    "papermill": {
     "duration": 0.167104,
     "end_time": "2023-09-13T18:56:04.448331",
     "exception": false,
     "start_time": "2023-09-13T18:56:04.281227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by CustomerID and Country to get the number of transactions per country for each customer\n",
    "customer_country = df.groupby(['CustomerID', 'Country']).size().reset_index(name='Number_of_Transactions')\n",
    "\n",
    "# Get the country with the maximum number of transactions for each customer (in case a customer has transactions from multiple countries)\n",
    "customer_main_country = customer_country.sort_values('Number_of_Transactions', ascending=False).drop_duplicates('CustomerID')\n",
    "\n",
    "# Create a binary column indicating whether the customer is from the UK or not\n",
    "customer_main_country['Is_UK'] = customer_main_country['Country'].apply(lambda x: 1 if x == 'United Kingdom' else 0)\n",
    "\n",
    "# Merge this data with our customer_data dataframe\n",
    "customer_data = pd.merge(customer_data, customer_main_country[['CustomerID', 'Is_UK']], on='CustomerID', how='left')\n",
    "\n",
    "# Display the first few rows of the customer_data dataframe\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172aabff",
   "metadata": {
    "papermill": {
     "duration": 0.081428,
     "end_time": "2023-09-13T18:56:04.601164",
     "exception": false,
     "start_time": "2023-09-13T18:56:04.519736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display feature distribution\n",
    "customer_data['Is_UK'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54878de8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.068165,
     "end_time": "2023-09-13T18:56:04.736100",
     "exception": false,
     "start_time": "2023-09-13T18:56:04.667935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cancellation_insights\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 4.5 |</span><span style='color:#ff6200'> Cancellation Insights</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2c273",
   "metadata": {
    "papermill": {
     "duration": 0.264581,
     "end_time": "2023-09-13T18:56:05.202510",
     "exception": false,
     "start_time": "2023-09-13T18:56:04.937929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of transactions made by each customer\n",
    "total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
    "\n",
    "# Calculate the number of cancelled transactions for each customer\n",
    "cancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']\n",
    "cancellation_frequency = cancelled_transactions.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
    "cancellation_frequency.rename(columns={'InvoiceNo': 'Cancellation_Frequency'}, inplace=True)\n",
    "\n",
    "# Merge the Cancellation Frequency data into the customer_data dataframe\n",
    "customer_data = pd.merge(customer_data, cancellation_frequency, on='CustomerID', how='left')\n",
    "\n",
    "# Replace NaN values with 0 (for customers who have not cancelled any transaction)\n",
    "customer_data['Cancellation_Frequency'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the Cancellation Rate\n",
    "customer_data['Cancellation_Rate'] = customer_data['Cancellation_Frequency'] / total_transactions['InvoiceNo']\n",
    "\n",
    "# Display the first few rows of the customer_data dataframe\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c381571",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.065303,
     "end_time": "2023-09-13T18:56:05.333472",
     "exception": false,
     "start_time": "2023-09-13T18:56:05.268169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"seasonality_trends\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 4.6 |</span><span style='color:#ff6200'> Seasonality & Trends</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5720ea3",
   "metadata": {
    "papermill": {
     "duration": 0.835651,
     "end_time": "2023-09-13T18:56:06.367576",
     "exception": false,
     "start_time": "2023-09-13T18:56:05.531925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract month and year from InvoiceDate\n",
    "df['Year'] = df['InvoiceDate'].dt.year\n",
    "df['Month'] = df['InvoiceDate'].dt.month\n",
    "\n",
    "# Calculate monthly spending for each customer\n",
    "monthly_spending = df.groupby(['CustomerID', 'Year', 'Month'])['Total_Spend'].sum().reset_index()\n",
    "\n",
    "# Calculate Seasonal Buying Patterns: We are using monthly frequency as a proxy for seasonal buying patterns\n",
    "seasonal_buying_patterns = monthly_spending.groupby('CustomerID')['Total_Spend'].agg(['mean', 'std']).reset_index()\n",
    "seasonal_buying_patterns.rename(columns={'mean': 'Monthly_Spending_Mean', 'std': 'Monthly_Spending_Std'}, inplace=True)\n",
    "\n",
    "# Replace NaN values in Monthly_Spending_Std with 0, implying no variability for customers with single transaction month\n",
    "seasonal_buying_patterns['Monthly_Spending_Std'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate Trends in Spending \n",
    "# We are using the slope of the linear trend line fitted to the customer's spending over time as an indicator of spending trends\n",
    "def calculate_trend(spend_data):\n",
    "    # If there are more than one data points, we calculate the trend using linear regression\n",
    "    if len(spend_data) > 1:\n",
    "        x = np.arange(len(spend_data))\n",
    "        slope, _, _, _, _ = linregress(x, spend_data)\n",
    "        return slope\n",
    "    # If there is only one data point, no trend can be calculated, hence we return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the calculate_trend function to find the spending trend for each customer\n",
    "spending_trends = monthly_spending.groupby('CustomerID')['Total_Spend'].apply(calculate_trend).reset_index()\n",
    "spending_trends.rename(columns={'Total_Spend': 'Spending_Trend'}, inplace=True)\n",
    "\n",
    "# Merge the new features into the customer_data dataframe\n",
    "customer_data = pd.merge(customer_data, seasonal_buying_patterns, on='CustomerID')\n",
    "customer_data = pd.merge(customer_data, spending_trends, on='CustomerID')\n",
    "\n",
    "# Display the first few rows of the customer_data dataframe\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77c0cc-be10-4dfa-a037-fe76495d3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9cc9e",
   "metadata": {
    "papermill": {
     "duration": 0.103278,
     "end_time": "2023-09-13T18:56:06.675301",
     "exception": false,
     "start_time": "2023-09-13T18:56:06.572023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changing the data type of 'CustomerID' to string as it is a unique identifier and not used in mathematical operations\n",
    "customer_data['CustomerID'] = customer_data['CustomerID'].astype(str)\n",
    "\n",
    "# Convert data types of columns to optimal types\n",
    "customer_data = customer_data.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a945c",
   "metadata": {
    "papermill": {
     "duration": 0.102034,
     "end_time": "2023-09-13T18:56:06.846461",
     "exception": false,
     "start_time": "2023-09-13T18:56:06.744427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84886ab4",
   "metadata": {
    "papermill": {
     "duration": 0.09233,
     "end_time": "2023-09-13T18:56:07.006785",
     "exception": false,
     "start_time": "2023-09-13T18:56:06.914455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94aed6",
   "metadata": {
    "papermill": {
     "duration": 0.070253,
     "end_time": "2023-09-13T18:56:07.561122",
     "exception": false,
     "start_time": "2023-09-13T18:56:07.490869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 align=\"left\"><font color=#ff6200>Let's dive in!</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850905bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.069589,
     "end_time": "2023-09-13T18:56:07.701447",
     "exception": false,
     "start_time": "2023-09-13T18:56:07.631858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"outlier_detection\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 5 | Outlier Detection and Treatment</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067ae0a",
   "metadata": {
    "papermill": {
     "duration": 0.728447,
     "end_time": "2023-09-13T18:56:08.639790",
     "exception": false,
     "start_time": "2023-09-13T18:56:07.911343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing the IsolationForest model with a contamination parameter of 0.05\n",
    "model = IsolationForest(contamination=0.05, random_state=0)\n",
    "\n",
    "# Fitting the model on our dataset (converting DataFrame to NumPy to avoid warning)\n",
    "customer_data['Outlier_Scores'] = model.fit_predict(customer_data.iloc[:, 1:].to_numpy())\n",
    "\n",
    "# Creating a new column to identify outliers (1 for inliers and -1 for outliers)\n",
    "customer_data['Is_Outlier'] = [1 if x == -1 else 0 for x in customer_data['Outlier_Scores']]\n",
    "\n",
    "# Display the first few rows of the customer_data dataframe\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a156aa4",
   "metadata": {
    "papermill": {
     "duration": 0.637863,
     "end_time": "2023-09-13T18:56:09.494637",
     "exception": false,
     "start_time": "2023-09-13T18:56:08.856774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of inliers and outliers\n",
    "outlier_percentage = customer_data['Is_Outlier'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plotting the percentage of inliers and outliers\n",
    "plt.figure(figsize=(12, 4))\n",
    "outlier_percentage.plot(kind='barh', color='#ff6200')\n",
    "\n",
    "# Adding the percentage labels on the bars\n",
    "for index, value in enumerate(outlier_percentage):\n",
    "    plt.text(value, index, f'{value:.2f}%', fontsize=15)\n",
    "\n",
    "plt.title('Percentage of Inliers and Outliers')\n",
    "plt.xticks(ticks=np.arange(0, 115, 5))\n",
    "plt.xlabel('Percentage (%)')\n",
    "plt.ylabel('Is Outlier')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662a17f",
   "metadata": {
    "papermill": {
     "duration": 0.095994,
     "end_time": "2023-09-13T18:56:09.945790",
     "exception": false,
     "start_time": "2023-09-13T18:56:09.849796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate the outliers for analysis\n",
    "outliers_data = customer_data[customer_data['Is_Outlier'] == 1]\n",
    "\n",
    "# Remove the outliers from the main dataset\n",
    "customer_data_cleaned = customer_data[customer_data['Is_Outlier'] == 0]\n",
    "\n",
    "# Drop the 'Outlier_Scores' and 'Is_Outlier' columns\n",
    "customer_data_cleaned = customer_data_cleaned.drop(columns=['Outlier_Scores', 'Is_Outlier'])\n",
    "\n",
    "# Reset the index of the cleaned data\n",
    "customer_data_cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3e398",
   "metadata": {
    "papermill": {
     "duration": 0.085015,
     "end_time": "2023-09-13T18:56:10.246151",
     "exception": false,
     "start_time": "2023-09-13T18:56:10.161136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the number of rows in the cleaned customer dataset\n",
    "customer_data_cleaned.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8510b52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.07047,
     "end_time": "2023-09-13T18:56:10.388162",
     "exception": false,
     "start_time": "2023-09-13T18:56:10.317692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"correlation\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 6 | Correlation Analysis</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df88ef0",
   "metadata": {
    "papermill": {
     "duration": 1.245909,
     "end_time": "2023-09-13T18:56:11.852633",
     "exception": false,
     "start_time": "2023-09-13T18:56:10.606724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset background style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Calculate the correlation matrix excluding the 'CustomerID' column\n",
    "corr = customer_data_cleaned.drop(columns=['CustomerID']).corr()\n",
    "\n",
    "# Define a custom colormap\n",
    "colors = ['#ff6200', '#ffcaa8', 'white', '#ffcaa8', '#ff6200']\n",
    "my_cmap = LinearSegmentedColormap.from_list('custom_map', colors, N=256)\n",
    "\n",
    "# Create a mask to only show the lower triangle of the matrix (since it's mirrored around its \n",
    "# top-left to bottom-right diagonal)\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, mask=mask, cmap=my_cmap, annot=True, center=0, fmt='.2f', linewidths=2)\n",
    "plt.title('Correlation Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a07521c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.078379,
     "end_time": "2023-09-13T18:56:12.342655",
     "exception": false,
     "start_time": "2023-09-13T18:56:12.264276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"scaling\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 7 | Feature Scaling</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa1efd",
   "metadata": {
    "papermill": {
     "duration": 0.079113,
     "end_time": "2023-09-13T18:56:12.503824",
     "exception": false,
     "start_time": "2023-09-13T18:56:12.424711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(24, 14, 1); font-size:120%; text-align:left\">\n",
    "\n",
    "Before we move forward with the clustering and dimensionality reduction, it's imperative to scale our features. This step holds significant importance, especially in the context of distance-based algorithms like K-means and dimensionality reduction methods like PCA. Here's why:\n",
    "\n",
    "  - __For K-means Clustering__: K-means relies heavily on the concept of '__distance__' between data points to form clusters. When features are not on a similar scale, features with larger values can disproportionately influence the clustering outcome, potentially leading to incorrect groupings.\n",
    "  \n",
    "    \n",
    "  - __For PCA__: PCA aims to find the directions where the data varies the most. When features are not scaled, those with larger values might dominate these components, not accurately reflecting the underlying patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca390b0c",
   "metadata": {
    "papermill": {
     "duration": 0.081475,
     "end_time": "2023-09-13T18:56:12.665586",
     "exception": false,
     "start_time": "2023-09-13T18:56:12.584111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(29, 17, 1); font-size:120%; text-align:left\">\n",
    "<h3 align=\"left\"><font color=#ff6200>Methodology: </font></h3>\n",
    "    \n",
    "Therefore, to ensure a balanced influence on the model and to reveal the true patterns in the data, I am going to standardize our data, meaning transforming the features to have a mean of 0 and a standard deviation of 1. However, not all features require scaling. Here are the exceptions and the reasons why they are excluded:\n",
    "\n",
    "- __CustomerID__: This feature is just an identifier for the customers and does not contain any meaningful information for clustering.\n",
    "    \n",
    "    \n",
    "- __Is_UK__: This is a binary feature indicating whether the customer is from the UK or not. Since it already takes a value of 0 or 1, scaling it won't make any significant difference.\n",
    "    \n",
    "    \n",
    "- __Day_Of_Week__: This feature represents the most frequent day of the week that the customer made transactions. Since it's a categorical feature represented by integers (1 to 7), scaling it would not be necessary. __EU__ fazer cycling scaling da hora e disto (converter com a funcao sin)\n",
    "\n",
    "    \n",
    "I will proceed to scale the other features in the dataset to prepare it for PCA and K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2707ef",
   "metadata": {
    "papermill": {
     "duration": 0.130165,
     "end_time": "2023-09-13T18:56:12.878414",
     "exception": false,
     "start_time": "2023-09-13T18:56:12.748249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# List of columns that don't need to be scaled\n",
    "columns_to_exclude = ['CustomerID', 'Is_UK', 'Day_Of_Week', 'Hour']\n",
    "\n",
    "# List of columns that need to be scaled\n",
    "columns_to_scale = customer_data_cleaned.columns.difference(columns_to_exclude)\n",
    "\n",
    "# Copy the cleaned dataset\n",
    "customer_data_scaled = customer_data_cleaned.copy()\n",
    "\n",
    "# Applying the scaler to the necessary columns in the dataset\n",
    "customer_data_scaled[columns_to_scale] = scaler.fit_transform(customer_data_scaled[columns_to_scale])\n",
    "\n",
    "# Display the first few rows of the scaled data\n",
    "customer_data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50365b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de colunas cíclicas\n",
    "cyclic_columns = ['Day_Of_Week', 'Hour']  # Adicione mais se necessário\n",
    "\n",
    "# Criar cópia do dataset\n",
    "customer_data_scaled = customer_data_cleaned.copy()\n",
    "\n",
    "# Aplicar transformação cíclica com seno\n",
    "for col in cyclic_columns:\n",
    "    period = customer_data_scaled[col].max() + 1.5  # Definir o período correto\n",
    "    customer_data_scaled[col] = np.sin(2 * np.pi * customer_data_scaled[col] / period)\n",
    "\n",
    "# Visualizar os primeiros resultados\n",
    "customer_data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b170f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.079003,
     "end_time": "2023-09-13T18:56:13.040264",
     "exception": false,
     "start_time": "2023-09-13T18:56:12.961261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"pca\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 8 | Dimensionality Reduction</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24948d",
   "metadata": {
    "papermill": {
     "duration": 1.164794,
     "end_time": "2023-09-13T18:56:14.748669",
     "exception": false,
     "start_time": "2023-09-13T18:56:13.583875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting CustomerID as the index column\n",
    "customer_data_scaled.set_index('CustomerID', inplace=True)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA().fit(customer_data_scaled)\n",
    "\n",
    "# Calculate the Cumulative Sum of the Explained Variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Set the optimal k value (based on our analysis, we can choose 6)\n",
    "optimal_k = 6\n",
    "\n",
    "# Set seaborn plot style\n",
    "sns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')\n",
    "\n",
    "# Plot the cumulative explained variance against the number of components\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Bar chart for the explained variance of each component\n",
    "barplot = sns.barplot(x=list(range(1, len(cumulative_explained_variance) + 1)),\n",
    "                      y=explained_variance_ratio,\n",
    "                      color='#fcc36d',\n",
    "                      alpha=0.8)\n",
    "\n",
    "# Line plot for the cumulative explained variance\n",
    "lineplot, = plt.plot(range(0, len(cumulative_explained_variance)), cumulative_explained_variance,\n",
    "                     marker='o', linestyle='--', color='#ff6200', linewidth=2)\n",
    "\n",
    "# Plot optimal k value line\n",
    "optimal_k_line = plt.axvline(optimal_k - 1, color='red', linestyle='--', label=f'Optimal k value = {optimal_k}') \n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Explained Variance', fontsize=14)\n",
    "plt.title('Cumulative Variance vs. Number of Components', fontsize=18)\n",
    "\n",
    "# Customize ticks and legend\n",
    "plt.xticks(range(0, len(cumulative_explained_variance)))\n",
    "plt.legend(handles=[barplot.patches[0], lineplot, optimal_k_line],\n",
    "           labels=['Explained Variance of Each Component', 'Cumulative Explained Variance', f'Optimal k value = {optimal_k}'],\n",
    "           loc=(0.62, 0.1),\n",
    "           frameon=True,\n",
    "           framealpha=1.0,  \n",
    "           edgecolor='#ff6200')  \n",
    "\n",
    "# Display the variance values for both graphs on the plots\n",
    "x_offset = -0.3\n",
    "y_offset = 0.01\n",
    "for i, (ev_ratio, cum_ev_ratio) in enumerate(zip(explained_variance_ratio, cumulative_explained_variance)):\n",
    "    plt.text(i, ev_ratio, f\"{ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "    if i > 0:\n",
    "        plt.text(i + x_offset, cum_ev_ratio + y_offset, f\"{cum_ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.grid(axis='both')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a40979",
   "metadata": {
    "papermill": {
     "duration": 0.134162,
     "end_time": "2023-09-13T18:56:15.130666",
     "exception": false,
     "start_time": "2023-09-13T18:56:14.996504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a PCA object with 6 components\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "# Fitting and transforming the original data to the new PCA dataframe\n",
    "customer_data_pca = pca.fit_transform(customer_data_scaled)\n",
    "\n",
    "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
    "customer_data_pca = pd.DataFrame(customer_data_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
    "\n",
    "# Adding the CustomerID index back to the new PCA dataframe\n",
    "customer_data_pca.index = customer_data_scaled.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8befe",
   "metadata": {
    "papermill": {
     "duration": 0.103676,
     "end_time": "2023-09-13T18:56:15.396556",
     "exception": false,
     "start_time": "2023-09-13T18:56:15.292880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying the resulting dataframe based on the PCs\n",
    "customer_data_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07765030",
   "metadata": {
    "papermill": {
     "duration": 0.223598,
     "end_time": "2023-09-13T18:56:15.869472",
     "exception": false,
     "start_time": "2023-09-13T18:56:15.645874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to highlight the top 3 absolute values in each column of a dataframe\n",
    "def highlight_top3(column):\n",
    "    top3 = column.abs().nlargest(3).index\n",
    "    return ['background-color:  #ffeacc' if i in top3 else '' for i in column.index]\n",
    "\n",
    "# Create the PCA component DataFrame and apply the highlighting function\n",
    "pc_df = pd.DataFrame(pca.components_.T, columns=['PC{}'.format(i+1) for i in range(pca.n_components_)],  \n",
    "                     index=customer_data_scaled.columns)\n",
    "\n",
    "pc_df.style.apply(highlight_top3, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef0ce8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.085571,
     "end_time": "2023-09-13T18:56:16.038485",
     "exception": false,
     "start_time": "2023-09-13T18:56:15.952914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"kmeans\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 9 | K-Means Clustering</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb3286",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.083487,
     "end_time": "2023-09-13T18:56:16.538067",
     "exception": false,
     "start_time": "2023-09-13T18:56:16.454580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"optimal_k\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 9.1 |</span><span style='color:#ff6200'> Determining the Optimal Number of Clusters</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4299824",
   "metadata": {
    "papermill": {
     "duration": 0.08091,
     "end_time": "2023-09-13T18:56:16.872605",
     "exception": false,
     "start_time": "2023-09-13T18:56:16.791695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"elbow\"></a>\n",
    "## <b><span style='color:#fcc36d'>Step 9.1.1 |</span><span style='color:#ff6200'> Elbow Method</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10ee37",
   "metadata": {
    "papermill": {
     "duration": 0.080067,
     "end_time": "2023-09-13T18:56:17.032975",
     "exception": false,
     "start_time": "2023-09-13T18:56:16.952908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "\n",
    "<h3 align=\"left\"><font color=#ff6200>What is the Elbow Method?</font></h3>\n",
    "    \n",
    "The Elbow Method is a technique for identifying the ideal number of clusters in a dataset. It involves iterating through the data, generating clusters for various values of k. The k-means algorithm calculates the sum of squared distances between each data point and its assigned cluster centroid, known as the __inertia__ or __WCSS__ score. By plotting the inertia score against the k value, we create a graph that typically exhibits an elbow shape, hence the name \"__Elbow Method__\". The __elbow point__ represents the k-value where the reduction in inertia achieved by increasing k becomes negligible, indicating the optimal stopping point for the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430ecaf",
   "metadata": {
    "papermill": {
     "duration": 0.080474,
     "end_time": "2023-09-13T18:56:17.195239",
     "exception": false,
     "start_time": "2023-09-13T18:56:17.114765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "\n",
    "<h3 align=\"left\"><font color=#ff6200>Utilizing the YellowBrick Library</font></h3>\n",
    "\n",
    "In this section, I will employ the __YellowBrick__ library to facilitate the implementation of the __Elbow method__. YellowBrick, an extension of the Scikit-Learn API, is renowned for its ability to rapidly generate insightful visualizations in the field of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f9237",
   "metadata": {
    "papermill": {
     "duration": 18.47856,
     "end_time": "2023-09-13T18:56:35.752589",
     "exception": false,
     "start_time": "2023-09-13T18:56:17.274029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set plot style, and background color\n",
    "sns.set(style='darkgrid', rc={'axes.facecolor': '#fcf0dc'})\n",
    "\n",
    "# Set the color palette for the plot\n",
    "sns.set_palette(['#ff6200'])\n",
    "\n",
    "# Instantiate the clustering model with the specified parameters\n",
    "km = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
    "\n",
    "# Create a figure and axis with the desired size\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the model and range of k values, and disable the timing plot\n",
    "visualizer = KElbowVisualizer(km, k=(2, 15), timings=False, ax=ax)\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "visualizer.fit(customer_data_pca)\n",
    "\n",
    "# Finalize and render the figure\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fd1c7",
   "metadata": {
    "papermill": {
     "duration": 0.086402,
     "end_time": "2023-09-13T18:56:35.922228",
     "exception": false,
     "start_time": "2023-09-13T18:56:35.835826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "\n",
    "<h3 align=\"left\"><font color=#ff6200>Optimal k Value: Elbow Method Insights</font></h3>\n",
    "\n",
    "The optimal value of k for the KMeans clustering algorithm can be found at the __elbow point__. Using the YellowBrick library for the Elbow method, we observe that the suggested optimal k value is __5__. However, __we don't have a very distinct elbow point in this case__, which is common in real-world data. From the plot, we can see that the inertia continues to decrease significantly up to k=5, indicating that __the optimum value of k could be between 3 and 7__. To choose the best k within this range, we can employ the __silhouette analysis__, another cluster quality evaluation method. Additionally, incorporating business insights can help determine a practical k value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94118a4b",
   "metadata": {
    "papermill": {
     "duration": 0.086471,
     "end_time": "2023-09-13T18:56:36.092946",
     "exception": false,
     "start_time": "2023-09-13T18:56:36.006475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"silhouette\"></a>\n",
    "## <b><span style='color:#fcc36d'>Step 9.1.2 |</span><span style='color:#ff6200'> Silhouette Method</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1de89",
   "metadata": {
    "papermill": {
     "duration": 0.080453,
     "end_time": "2023-09-13T18:56:36.256900",
     "exception": false,
     "start_time": "2023-09-13T18:56:36.176447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "\n",
    "<h3 align=\"left\"><font color=#ff6200>What is the Silhouette Method?</font></h3>\n",
    "    \n",
    "The __Silhouette Method__ is an approach to find the optimal number of clusters in a dataset by evaluating the consistency within clusters and their separation from other clusters. It computes the __silhouette coefficient for each data point__, which measures how similar a point is to its own cluster compared to other clusters.\n",
    "\n",
    "____\n",
    "    \n",
    "<h3 align=\"left\"><font color=#ff6200>What is the Silhouette Coefficient?</font></h3>\n",
    "    \n",
    "To determine the silhouette coefficient for a given point i, follow these steps:\n",
    "\n",
    "* __Calculate a(i)__: Compute the average distance between point i and all other points within its cluster.\n",
    "* __Calculate b(i)__: Compute the average distance between point i and all points in the nearest cluster to its own.\n",
    "* __Compute the silhouette coefficient__, s(i), for point i using the following formula: \n",
    "    \n",
    "    $$ s(i) = \\frac{b(i) - a(i)}{\\max(b(i), a(i))} $$\n",
    "    \n",
    "__Note:__ The silhouette coefficient quantifies the similarity of a point to its own cluster (cohesion) relative to its separation from other clusters. This value ranges from -1 to 1, with higher values signifying that the point is well aligned with its cluster and has a low similarity to neighboring clusters.    \n",
    "\n",
    "____\n",
    "    \n",
    "<h3 align=\"left\"><font color=#ff6200>What is the Silhouette Score?</font></h3>\n",
    "    \n",
    "The __silhouette score__ is the __average silhouette coefficient__ calculated for all data points in a dataset. It provides an overall assessment of the clustering quality, taking into account both cohesion within clusters and separation between clusters. A higher silhouette score indicates a better clustering configuration.    \n",
    "    \n",
    "____\n",
    "       \n",
    "<h3 align=\"left\"><font color=#ff6200>What are the Advantages of Silhouette Method over the Elbow Method?</font></h3>\n",
    "    \n",
    "* The __Silhouette Method__ evaluates cluster quality by considering __both__ the __cohesion within clusters__ and their __separation__ from other clusters. This provides a more comprehensive measure of clustering performance compared to the __Elbow Method__, which only considers the __inertia__ (sum of squared distances within clusters).\n",
    "\n",
    "\n",
    "* The __Silhouette Method__ produces a silhouette score that directly quantifies the quality of clustering, making it easier to compare different values of k. In contrast, the __Elbow Method__ relies on the subjective interpretation of the elbow point, which can be less reliable in cases where the plot does not show a clear elbow.\n",
    "\n",
    "    \n",
    "* The __Silhouette Method__ generates a visual representation of silhouette coefficients for each data point, allowing for easier identification of fluctuations and outliers within clusters. This helps in determining the optimal number of clusters with higher confidence, as opposed to the __Elbow Method__, which relies on visual inspection of the inertia plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43ed65",
   "metadata": {
    "papermill": {
     "duration": 0.080951,
     "end_time": "2023-09-13T18:56:36.422273",
     "exception": false,
     "start_time": "2023-09-13T18:56:36.341322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "\n",
    "<h3 align=\"left\"><font color=#ff6200>Methodology</font></h3>\n",
    "    \n",
    "In the following analysis:\n",
    "\n",
    "- I will initially choose a range of 2-6 for the number of clusters (k) based on the Elbow method from the previous section. Next, I will plot __Silhouette scores__ for each k value to determine the one with the highest score.\n",
    "\n",
    "\n",
    "- Subsequently, to fine-tune the selection of the most appropriate k, I will generate __Silhouette plots__ that visually display the __silhouette coefficients for each data point within various clusters__.\n",
    "\n",
    "\n",
    "The __YellowBrick__ library will be utilized once again to create these plots and facilitate a comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24135687",
   "metadata": {
    "papermill": {
     "duration": 0.105705,
     "end_time": "2023-09-13T18:56:36.609774",
     "exception": false,
     "start_time": "2023-09-13T18:56:36.504069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def silhouette_analysis(df, start_k, stop_k, figsize=(15, 16)):\n",
    "    \"\"\"\n",
    "    Perform Silhouette analysis for a range of k values and visualize the results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the size of the figure\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Create a grid with (stop_k - start_k + 1) rows and 2 columns\n",
    "    grid = gridspec.GridSpec(stop_k - start_k + 1, 2)\n",
    "\n",
    "    # Assign the first plot to the first row and both columns\n",
    "    first_plot = plt.subplot(grid[0, :])\n",
    "\n",
    "    # First plot: Silhouette scores for different k values\n",
    "    sns.set_palette(['darkorange'])\n",
    "\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Iterate through the range of k values\n",
    "    for k in range(start_k, stop_k + 1):\n",
    "        km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
    "        km.fit(df)\n",
    "        labels = km.predict(df)\n",
    "        score = silhouette_score(df, labels)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "    best_k = start_k + silhouette_scores.index(max(silhouette_scores))\n",
    "\n",
    "    plt.plot(range(start_k, stop_k + 1), silhouette_scores, marker='o')\n",
    "    plt.xticks(range(start_k, stop_k + 1))\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title('Average Silhouette Score for Different k Values', fontsize=15)\n",
    "\n",
    "    # Add the optimal k value text to the plot\n",
    "    optimal_k_text = f'The k value with the highest Silhouette score is: {best_k}'\n",
    "    plt.text(10, 0.23, optimal_k_text, fontsize=12, verticalalignment='bottom', \n",
    "             horizontalalignment='left', bbox=dict(facecolor='#fcc36d', edgecolor='#ff6200', boxstyle='round, pad=0.5'))\n",
    "             \n",
    "\n",
    "    # Second plot (subplot): Silhouette plots for each k value\n",
    "    colors = sns.color_palette(\"bright\")\n",
    "\n",
    "    for i in range(start_k, stop_k + 1):    \n",
    "        km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
    "        row_idx, col_idx = divmod(i - start_k, 2)\n",
    "\n",
    "        # Assign the plots to the second, third, and fourth rows\n",
    "        ax = plt.subplot(grid[row_idx + 1, col_idx])\n",
    "\n",
    "        visualizer = SilhouetteVisualizer(km, colors=colors, ax=ax)\n",
    "        visualizer.fit(df)\n",
    "\n",
    "        # Add the Silhouette score text to the plot\n",
    "        score = silhouette_score(df, km.labels_)\n",
    "        ax.text(0.97, 0.02, f'Silhouette Score: {score:.2f}', fontsize=12, \\\n",
    "                ha='right', transform=ax.transAxes, color='red')\n",
    "\n",
    "        ax.set_title(f'Silhouette Plot for {i} Clusters', fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971cf0b",
   "metadata": {
    "papermill": {
     "duration": 48.801465,
     "end_time": "2023-09-13T18:57:25.496889",
     "exception": false,
     "start_time": "2023-09-13T18:56:36.695424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "silhouette_analysis(customer_data_pca, 3, 12, figsize=(20, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad805a5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.093555,
     "end_time": "2023-09-13T18:57:26.063850",
     "exception": false,
     "start_time": "2023-09-13T18:57:25.970295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"kmeans_model\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 9.2 |</span><span style='color:#ff6200'> Clustering Model - K-means</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3238b",
   "metadata": {
    "papermill": {
     "duration": 1.408565,
     "end_time": "2023-09-13T18:57:27.752578",
     "exception": false,
     "start_time": "2023-09-13T18:57:26.344013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply KMeans clustering using the optimal k\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
    "kmeans.fit(customer_data_pca)\n",
    "\n",
    "# Get the frequency of each cluster\n",
    "cluster_frequencies = Counter(kmeans.labels_)\n",
    "\n",
    "# Create a mapping from old labels to new labels based on frequency\n",
    "label_mapping = {label: new_label for new_label, (label, _) in \n",
    "                 enumerate(cluster_frequencies.most_common())}\n",
    "\n",
    "# Reverse the mapping to assign labels as per your criteria\n",
    "label_mapping = {v: k for k, v in {4: 1, 3: 0, 2: 4, 1: 3, 0: 2}.items()}\n",
    "\n",
    "# Apply the mapping to get the new labels\n",
    "new_labels = np.array([label_mapping[label] for label in kmeans.labels_])\n",
    "\n",
    "# Append the new cluster labels back to the original dataset\n",
    "customer_data_cleaned['cluster'] = new_labels\n",
    "\n",
    "# Append the new cluster labels to the PCA version of the dataset\n",
    "customer_data_pca['cluster'] = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac520a",
   "metadata": {
    "papermill": {
     "duration": 0.12762,
     "end_time": "2023-09-13T18:57:27.973860",
     "exception": false,
     "start_time": "2023-09-13T18:57:27.846240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the original dataframe\n",
    "customer_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e7a50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.093338,
     "end_time": "2023-09-13T18:57:28.163349",
     "exception": false,
     "start_time": "2023-09-13T18:57:28.070011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 10 | Clustering Evaluation</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bd1a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.09431,
     "end_time": "2023-09-13T18:57:28.548639",
     "exception": false,
     "start_time": "2023-09-13T18:57:28.454329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3d_visualization\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 10.1 |</span><span style='color:#ff6200'>  3D Visualization of Top Principal Components</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9423286",
   "metadata": {
    "papermill": {
     "duration": 0.104131,
     "end_time": "2023-09-13T18:57:28.937718",
     "exception": false,
     "start_time": "2023-09-13T18:57:28.833587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the color scheme for the clusters (RGB order)\n",
    "colors = ['#e8000b', '#1ac938', '#023eff', \"#FFC0CB\", \"#964B00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33fd0a",
   "metadata": {
    "papermill": {
     "duration": 0.482553,
     "end_time": "2023-09-13T18:57:29.514649",
     "exception": false,
     "start_time": "2023-09-13T18:57:29.032096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create separate data frames for each cluster\n",
    "cluster_0 = customer_data_pca[customer_data_pca['cluster'] == 0]\n",
    "cluster_1 = customer_data_pca[customer_data_pca['cluster'] == 1]\n",
    "cluster_2 = customer_data_pca[customer_data_pca['cluster'] == 2]\n",
    "cluster_3 = customer_data_pca[customer_data_pca['cluster'] == 3]\n",
    "cluster_4 = customer_data_pca[customer_data_pca['cluster'] == 4]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points for each cluster separately and specify the color\n",
    "fig.add_trace(go.Scatter3d(x=cluster_0['PC1'], y=cluster_0['PC2'], z=cluster_0['PC3'], \n",
    "                           mode='markers', marker=dict(color=colors[0], size=5, opacity=0.4), name='Cluster 0'))\n",
    "fig.add_trace(go.Scatter3d(x=cluster_1['PC1'], y=cluster_1['PC2'], z=cluster_1['PC3'], \n",
    "                           mode='markers', marker=dict(color=colors[1], size=5, opacity=0.4), name='Cluster 1'))\n",
    "fig.add_trace(go.Scatter3d(x=cluster_2['PC1'], y=cluster_2['PC2'], z=cluster_2['PC3'], \n",
    "                           mode='markers', marker=dict(color=colors[2], size=5, opacity=0.4), name='Cluster 2'))\n",
    "fig.add_trace(go.Scatter3d(x=cluster_3['PC1'], y=cluster_3['PC2'], z=cluster_3['PC3'], \n",
    "                           mode='markers', marker=dict(color=colors[3], size=5, opacity=0.4), name='Cluster 3'))\n",
    "fig.add_trace(go.Scatter3d(x=cluster_4['PC1'], y=cluster_4['PC2'], z=cluster_4['PC3'], \n",
    "                           mode='markers', marker=dict(color=colors[4], size=5, opacity=0.4), name='Cluster 4'))\n",
    "\n",
    "# Set the title and layout details\n",
    "fig.update_layout(\n",
    "    title=dict(text='3D Visualization of Customer Clusters in PCA Space', x=0.5),\n",
    "    scene=dict(\n",
    "        xaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC1'),\n",
    "        yaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC2'),\n",
    "        zaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC3'),\n",
    "    ),\n",
    "    width=900,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b78551",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.254792,
     "end_time": "2023-09-13T18:57:29.925873",
     "exception": false,
     "start_time": "2023-09-13T18:57:29.671081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cluster_distributuion\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 10.2 |</span><span style='color:#ff6200'>  Cluster Distribution Visualization</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac250d9",
   "metadata": {
    "papermill": {
     "duration": 0.155618,
     "end_time": "2023-09-13T18:57:30.236070",
     "exception": false,
     "start_time": "2023-09-13T18:57:30.080452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "    \n",
    "I am going to utilize a bar plot to visualize the percentage of customers in each cluster, which helps in understanding if the clusters are balanced and significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c617e70",
   "metadata": {
    "papermill": {
     "duration": 0.55019,
     "end_time": "2023-09-13T18:57:30.939968",
     "exception": false,
     "start_time": "2023-09-13T18:57:30.389778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of customers in each cluster\n",
    "cluster_percentage = (customer_data_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
    "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
    "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n",
    "\n",
    "# Adding percentages on the bars\n",
    "for index, value in enumerate(cluster_percentage['Percentage']):\n",
    "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
    "\n",
    "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
    "plt.xticks(ticks=np.arange(0, 50, 5))\n",
    "plt.xlabel('Percentage (%)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236e9d8",
   "metadata": {
    "papermill": {
     "duration": 0.152868,
     "end_time": "2023-09-13T18:57:31.244931",
     "exception": false,
     "start_time": "2023-09-13T18:57:31.092063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "<h3 align=\"left\"><font color=#ff6200>Inference</font></h3>\n",
    "    \n",
    "The distribution of customers across the clusters, as depicted by the bar plot, suggests a fairly balanced distribution with clusters 0 and 1 holding around 41% of customers each and cluster 2 accommodating approximately 18% of the customers. \n",
    "\n",
    "This balanced distribution indicates that our clustering process has been largely successful in identifying meaningful patterns within the data, rather than merely grouping noise or outliers. It implies that each cluster represents a substantial and distinct segment of the customer base, thereby offering valuable insights for future business strategies.\n",
    "\n",
    "Moreover, the fact that no cluster contains a very small percentage of customers, assures us that each cluster is significant and not just representing outliers or noise in the data. This setup allows for a more nuanced understanding and analysis of different customer segments, facilitating effective and informed decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c91c1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.155889,
     "end_time": "2023-09-13T18:57:31.558512",
     "exception": false,
     "start_time": "2023-09-13T18:57:31.402623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"evaluations_metrics\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 10.3 |</span><span style='color:#ff6200'> Evaluation Metrics</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff66d31",
   "metadata": {
    "papermill": {
     "duration": 0.155537,
     "end_time": "2023-09-13T18:57:31.873252",
     "exception": false,
     "start_time": "2023-09-13T18:57:31.717715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(49, 30, 3); font-size:120%; text-align:left\">\n",
    "\n",
    "To further scrutinize the quality of our clustering, I will employ the following metrics:\n",
    "\n",
    "- __Silhouette Score__: A measure to evaluate the separation distance between the clusters. Higher values indicate better cluster separation. It ranges from -1 to 1.\n",
    "    \n",
    "    \n",
    "- __Calinski Harabasz Score__: This score is used to evaluate the dispersion between and within clusters. A higher score indicates better defined clusters.\n",
    "\n",
    "    \n",
    "- __Davies Bouldin Score__: It assesses the average similarity between each cluster and its most similar cluster. Lower values indicate better cluster separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85d0d4",
   "metadata": {
    "papermill": {
     "duration": 0.555512,
     "end_time": "2023-09-13T18:57:32.585864",
     "exception": false,
     "start_time": "2023-09-13T18:57:32.030352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute number of customers\n",
    "num_observations = len(customer_data_pca)\n",
    "\n",
    "# Separate the features and the cluster labels\n",
    "X = customer_data_pca.drop('cluster', axis=1)\n",
    "clusters = customer_data_pca['cluster']\n",
    "\n",
    "# Compute the metrics\n",
    "sil_score = silhouette_score(X, clusters)\n",
    "calinski_score = calinski_harabasz_score(X, clusters)\n",
    "davies_score = davies_bouldin_score(X, clusters)\n",
    "\n",
    "# Create a table to display the metrics and the number of observations\n",
    "table_data = [\n",
    "    [\"Number of Observations\", num_observations],\n",
    "    [\"Silhouette Score\", sil_score],\n",
    "    [\"Calinski Harabasz Score\", calinski_score],\n",
    "    [\"Davies Bouldin Score\", davies_score]\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1305d",
   "metadata": {
    "papermill": {
     "duration": 0.154675,
     "end_time": "2023-09-13T18:57:32.949852",
     "exception": false,
     "start_time": "2023-09-13T18:57:32.795177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "<h3 align=\"left\"><font color=#ff6200>Clustering Quality Inference</font></h3>\n",
    "    \n",
    "    \n",
    "- The __Silhouette Score__ of approximately 0.236, although not close to 1, still indicates a fair amount of separation between the clusters. It suggests that the clusters are somewhat distinct, but there might be slight overlaps between them. Generally, a score closer to 1 would be ideal, indicating more distinct and well-separated clusters.\n",
    "\n",
    "    \n",
    "- The __Calinski Harabasz Score__ is 1257.17, which is considerably high, indicating that the clusters are well-defined. A higher score in this metric generally signals better cluster definitions, thus implying that our clustering has managed to find substantial structure in the data.\n",
    "\n",
    "    \n",
    "- The __Davies Bouldin Score__ of 1.37 is a reasonable score, indicating a moderate level of similarity between each cluster and its most similar one. A lower score is generally better as it indicates less similarity between clusters, and thus, our score here suggests a decent separation between the clusters.\n",
    "\n",
    "\n",
    "In conclusion, the metrics suggest that the clustering is of good quality, with clusters being well-defined and fairly separated. However, there might still be room for further optimization to enhance cluster separation and definition, potentially by trying other clustering and dimensionality reduction algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7544c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.157282,
     "end_time": "2023-09-13T18:57:33.262175",
     "exception": false,
     "start_time": "2023-09-13T18:57:33.104893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"profiling\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 11 | Cluster Analysis and Profiling</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343622b",
   "metadata": {
    "papermill": {
     "duration": 0.15762,
     "end_time": "2023-09-13T18:57:33.573094",
     "exception": false,
     "start_time": "2023-09-13T18:57:33.415474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "    \n",
    "In this section, I am going to analyze the characteristics of each cluster to understand the distinct behaviors and preferences of different customer segments and also profile each cluster to identify the key traits that define the customers in each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18fe95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.154276,
     "end_time": "2023-09-13T18:57:33.907463",
     "exception": false,
     "start_time": "2023-09-13T18:57:33.753187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"radar_chart\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 11.1 |</span><span style='color:#ff6200'> Radar Chart Approach</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf4657",
   "metadata": {
    "papermill": {
     "duration": 0.155714,
     "end_time": "2023-09-13T18:57:34.221443",
     "exception": false,
     "start_time": "2023-09-13T18:57:34.065729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "    \n",
    "First of all, I am going to create radar charts to visualize the centroid values of each cluster across different features. This can give a quick visual comparison of the profiles of different clusters.To construct the radar charts, it's essential to first compute the centroid for each cluster. This centroid represents the mean value for all features within a specific cluster. Subsequently, I will display these centroids on the radar charts, facilitating a clear visualization of the central tendencies of each feature across the various clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ebcad",
   "metadata": {
    "papermill": {
     "duration": 1.51518,
     "end_time": "2023-09-13T18:57:35.893853",
     "exception": false,
     "start_time": "2023-09-13T18:57:34.378673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definir 'CustomerID' como índice\n",
    "df_customer = customer_data_cleaned.set_index('CustomerID')\n",
    "\n",
    "# Padronizar os dados (excluindo a coluna 'cluster')\n",
    "scaler = StandardScaler()\n",
    "df_customer_standardized = scaler.fit_transform(df_customer.drop(columns=['cluster'], axis=1))\n",
    "\n",
    "# Criar um novo DataFrame padronizado e adicionar a coluna de cluster de volta\n",
    "df_customer_standardized = pd.DataFrame(df_customer_standardized, columns=df_customer.columns[:-1], index=df_customer.index)\n",
    "df_customer_standardized['cluster'] = df_customer['cluster']\n",
    "\n",
    "# Calcular os centroides de cada cluster\n",
    "cluster_centroids = df_customer_standardized.groupby('cluster').mean()\n",
    "\n",
    "# Função para criar um gráfico de radar\n",
    "def create_radar_chart(ax, angles, data, color, cluster):\n",
    "    ax.fill(angles, data, color=color, alpha=0.4)\n",
    "    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')\n",
    "    ax.set_title(f'Cluster {cluster}', size=15, color=color, y=1.1)\n",
    "\n",
    "# Definir os rótulos das variáveis\n",
    "labels = np.array(cluster_centroids.columns)\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Calcular os ângulos para cada eixo do gráfico de radar\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "# Completar o ciclo para fechar o gráfico de radar\n",
    "labels = np.concatenate((labels, [labels[0]]))\n",
    "angles += angles[:1]\n",
    "\n",
    "# Criar a figura para 5 clusters\n",
    "fig, ax = plt.subplots(figsize=(25, 10), subplot_kw=dict(polar=True), nrows=1, ncols=5)\n",
    "\n",
    "\n",
    "# Criar gráficos de radar para cada cluster\n",
    "for i, color in enumerate(colors):\n",
    "    data = cluster_centroids.loc[i].tolist()\n",
    "    data += data[:1]  # Fechar o loop\n",
    "    create_radar_chart(ax[i], angles, data, color, i)\n",
    "    ax[i].set_xticks(angles[:-1])\n",
    "    ax[i].set_xticklabels(labels[:-1])\n",
    "\n",
    "# Ajustar layout e exibir o gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c3834",
   "metadata": {
    "papermill": {
     "duration": 0.164772,
     "end_time": "2023-09-13T18:57:36.220301",
     "exception": false,
     "start_time": "2023-09-13T18:57:36.055529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(33, 23, 2); font-size:120%; text-align:left\">\n",
    "    \n",
    "<h2 align=\"left\"><font color=#ff6200>Customer Profiles Derived from Radar Chart Analysis</font></h2>\n",
    "    \n",
    "<h3 align=\"left\"><font color=red>Cluster 0 (Red Chart):</font></h3>\n",
    "\n",
    "🎯 Profile: __Sporadic Shoppers with a Preference for Weekend Shopping__  \n",
    "\n",
    "- Customers in this cluster tend to spend less, with a lower number of transactions and products purchased.  \n",
    "- They have a slight tendency to shop during the weekends, as indicated by the very high `Day_of_Week` value.  \n",
    "- Their spending trend is relatively stable but on the lower side, and they have a low monthly spending variation (low `Monthly_Spending_Std`).  \n",
    "- These customers have not engaged in many cancellations, showing a low cancellation frequency and rate.  \n",
    "- The average transaction value is on the lower side, indicating that when they do shop, they tend to spend less per transaction.  \n",
    "\n",
    "____\n",
    "    \n",
    "<h3 align=\"left\"><font color=green>Cluster 1 (Green Chart):</font></h3>    \n",
    " \n",
    "🎯 Profile: __Infrequent Big Spenders with a High Spending Trend__  \n",
    "    \n",
    "- Customers in this cluster show a moderate level of spending, but their transactions are not very frequent, as indicated by the high `Days_Since_Last_Purchase` and `Average_Days_Between_Purchases`.  \n",
    "- They have a very high spending trend, indicating that their spending has been increasing over time.  \n",
    "- These customers prefer shopping late in the day, as indicated by the high `Hour` value, and they mainly reside in the UK.  \n",
    "- They have a tendency to cancel a moderate number of transactions, with a medium cancellation frequency and rate.  \n",
    "- Their average transaction value is relatively high, meaning that when they shop, they tend to make substantial purchases.  \n",
    "\n",
    "____\n",
    "    \n",
    "<h3 align=\"left\"><font color=blue>Cluster 2 (Blue Chart):</font></h3>   \n",
    "\n",
    "🎯 Profile: __Frequent High-Spenders with a High Rate of Cancellations__\n",
    "    \n",
    "- Customers in this cluster are high spenders with a very high total spend, and they purchase a wide variety of unique products.  \n",
    "- They engage in frequent transactions, but also have a high cancellation frequency and rate.  \n",
    "- These customers have a very low average time between purchases, and they tend to shop early in the day (low `Hour` value).  \n",
    "- Their monthly spending shows high variability, indicating that their spending patterns might be less predictable compared to other clusters.  \n",
    "- Despite their high spending, they show a low spending trend, suggesting that their high spending levels might be decreasing over time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1949f7f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.162758,
     "end_time": "2023-09-13T18:57:36.542206",
     "exception": false,
     "start_time": "2023-09-13T18:57:36.379448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"histogram\"></a>\n",
    "# <b><span style='color:#fcc36d'>Step 11.2 |</span><span style='color:#ff6200'> Histogram Chart Approach</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704973d0",
   "metadata": {
    "papermill": {
     "duration": 0.162619,
     "end_time": "2023-09-13T18:57:36.866049",
     "exception": false,
     "start_time": "2023-09-13T18:57:36.703430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "    \n",
    "To validate the profiles identified from the radar charts, we can plot histograms for each feature segmented by the cluster labels. These histograms will allow us to visually inspect the distribution of feature values within each cluster, thereby confirming or refining the profiles we have created based on the radar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23c83d",
   "metadata": {
    "papermill": {
     "duration": 18.726601,
     "end_time": "2023-09-13T18:57:55.787377",
     "exception": false,
     "start_time": "2023-09-13T18:57:37.060776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot histograms for each feature segmented by the clusters\n",
    "features = customer_data_cleaned.columns[1:-1]\n",
    "clusters = customer_data_cleaned['cluster'].unique()\n",
    "clusters.sort()\n",
    "\n",
    "print(clusters)\n",
    "\n",
    "# Setting up the subplots\n",
    "n_rows = len(features)\n",
    "n_cols = len(clusters)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n",
    "\n",
    "# Plotting histograms\n",
    "for i, feature in enumerate(features):\n",
    "    for j, cluster in enumerate(clusters):\n",
    "        data = customer_data_cleaned[customer_data_cleaned['cluster'] == cluster][feature]\n",
    "        axes[i, j].hist(data, bins=20, color=colors[j], edgecolor='w', alpha=0.7)\n",
    "        axes[i, j].set_title(f'Cluster {cluster} - {feature}', fontsize=15)\n",
    "        axes[i, j].set_xlabel('')\n",
    "        axes[i, j].set_ylabel('')\n",
    "\n",
    "# Adjusting layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29b465",
   "metadata": {
    "papermill": {
     "duration": 0.172749,
     "end_time": "2023-09-13T18:57:56.133658",
     "exception": false,
     "start_time": "2023-09-13T18:57:55.960909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n",
    "\n",
    "The detailed insights from the histograms provide a more nuanced understanding of each cluster, helping in refining the profiles to represent the customer behaviors more accurately. Based on the detailed analysis from both the radar charts and the histograms, here are the refined profiles and titles for each cluster:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1e8c2",
   "metadata": {
    "papermill": {
     "duration": 0.172403,
     "end_time": "2023-09-13T18:57:56.480300",
     "exception": false,
     "start_time": "2023-09-13T18:57:56.307897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://github.com/FarzadNekouee/Retail_Customer_Segmentation_Recommendation_System/blob/master/profiles.png?raw=true\" width=\"2400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d9b66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.169573,
     "end_time": "2023-09-13T18:57:56.822756",
     "exception": false,
     "start_time": "2023-09-13T18:57:56.653183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"recommendation_system\"></a>\n",
    "# <p style=\"background-color: #ff6200; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 12 | Recommendation System</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c735ac6",
   "metadata": {
    "papermill": {
     "duration": 0.177632,
     "end_time": "2023-09-13T18:57:57.172146",
     "exception": false,
     "start_time": "2023-09-13T18:57:56.994514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; padding: 15px; background-color:rgb(32, 19, 1); font-size:120%; text-align:left\">\n",
    "\n",
    "In the final phase of this project, I am set to develop a recommendation system to enhance the online shopping experience. This system will suggest products to customers based on the purchasing patterns prevalent in their respective clusters. Earlier in the project, during the customer data preparation stage, I isolated a small fraction (5%) of the customers identified as outliers and reserved them in a separate dataset called `outliers_data`.\n",
    "\n",
    "Now, focusing on the core 95% of the customer group, I analyze the cleansed customer data to pinpoint the top-selling products within each cluster. Leveraging this information, the system will craft personalized recommendations, suggesting __the top three products__ popular within their cluster that they have not yet purchased. This not only facilitates targeted marketing strategies but also enriches the personal shopping experience, potentially boosting sales. For the outlier group, a basic approach could be to recommend random products, as a starting point to engage them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74145455",
   "metadata": {
    "papermill": {
     "duration": 11.716599,
     "end_time": "2023-09-13T18:58:09.060770",
     "exception": false,
     "start_time": "2023-09-13T18:57:57.344171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Extract the CustomerIDs of the outliers and remove their transactions from the main dataframe\n",
    "outlier_customer_ids = outliers_data['CustomerID'].astype('float').unique()\n",
    "df_filtered = df[~df['CustomerID'].isin(outlier_customer_ids)]\n",
    "\n",
    "# Step 2: Ensure consistent data type for CustomerID across both dataframes before merging\n",
    "customer_data_cleaned['CustomerID'] = customer_data_cleaned['CustomerID'].astype('float')\n",
    "\n",
    "# Step 3: Merge the transaction data with the customer data to get the cluster information for each transaction\n",
    "merged_data = df_filtered.merge(customer_data_cleaned[['CustomerID', 'cluster']], on='CustomerID', how='inner')\n",
    "\n",
    "# Step 4: Identify the top 10 best-selling products in each cluster based on the total quantity sold\n",
    "best_selling_products = merged_data.groupby(['cluster', 'StockCode', 'Description'])['Quantity'].sum().reset_index()\n",
    "best_selling_products = best_selling_products.sort_values(by=['cluster', 'Quantity'], ascending=[True, False])\n",
    "top_products_per_cluster = best_selling_products.groupby('cluster').head(10)\n",
    "\n",
    "# Step 5: Create a record of products purchased by each customer in each cluster\n",
    "customer_purchases = merged_data.groupby(['CustomerID', 'cluster', 'StockCode'])['Quantity'].sum().reset_index()\n",
    "\n",
    "# Step 6: Generate recommendations for each customer in each cluster\n",
    "recommendations = []\n",
    "for cluster in top_products_per_cluster['cluster'].unique():\n",
    "    top_products = top_products_per_cluster[top_products_per_cluster['cluster'] == cluster]\n",
    "    customers_in_cluster = customer_data_cleaned[customer_data_cleaned['cluster'] == cluster]['CustomerID']\n",
    "    \n",
    "    for customer in customers_in_cluster:\n",
    "        # Identify products already purchased by the customer\n",
    "        customer_purchased_products = customer_purchases[(customer_purchases['CustomerID'] == customer) & \n",
    "                                                         (customer_purchases['cluster'] == cluster)]['StockCode'].tolist()\n",
    "        \n",
    "        # Find top 3 products in the best-selling list that the customer hasn't purchased yet\n",
    "        top_products_not_purchased = top_products[~top_products['StockCode'].isin(customer_purchased_products)]\n",
    "        top_3_products_not_purchased = top_products_not_purchased.head(3)\n",
    "        \n",
    "        # Append the recommendations to the list\n",
    "        recommendations.append([customer, cluster] + top_3_products_not_purchased[['StockCode', 'Description']].values.flatten().tolist())\n",
    "\n",
    "# Step 7: Create a dataframe from the recommendations list and merge it with the original customer data\n",
    "recommendations_df = pd.DataFrame(recommendations, columns=['CustomerID', 'cluster', 'Rec1_StockCode', 'Rec1_Description', \\\n",
    "                                                 'Rec2_StockCode', 'Rec2_Description', 'Rec3_StockCode', 'Rec3_Description'])\n",
    "customer_data_with_recommendations = customer_data_cleaned.merge(recommendations_df, on=['CustomerID', 'cluster'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a101bd3",
   "metadata": {
    "papermill": {
     "duration": 0.199648,
     "end_time": "2023-09-13T18:58:09.436781",
     "exception": false,
     "start_time": "2023-09-13T18:58:09.237133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display 10 random rows from the customer_data_with_recommendations dataframe\n",
    "customer_data_with_recommendations.set_index('CustomerID').iloc[:, -6:].sample(10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c633c37",
   "metadata": {
    "papermill": {
     "duration": 0.173764,
     "end_time": "2023-09-13T18:58:09.788825",
     "exception": false,
     "start_time": "2023-09-13T18:58:09.615061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center; border-radius: 10px; padding: 20px; background-color: #ffeacc; font-size: 120%; text-align: center;\">\n",
    "\n",
    "<strong>🎯 If you need more information or want to explore the code, feel free to visit the project repository on <a href=\"https://github.com/FarzadNekouee/Retail_Customer_Segmentation_Recommendation_System\">GitHub</a> 🎯</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a36a15",
   "metadata": {
    "papermill": {
     "duration": 0.176471,
     "end_time": "2023-09-13T18:58:10.140711",
     "exception": false,
     "start_time": "2023-09-13T18:58:09.964240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 align=\"left\"><font color='#ff6200'>Best Regards!</font></h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 168.09806,
   "end_time": "2023-09-13T18:58:13.013821",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-13T18:55:24.915761",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
